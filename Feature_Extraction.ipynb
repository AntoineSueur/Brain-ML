{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Traiter le cas de mots inconnus pour les word embeddings\n",
    "- Optimiser dataloader\n",
    "- Trouver autre dataset\n",
    "- calculer sparsité\n",
    "- Comparer plusieurs sparsité / autoencodeurs under/overcomplete/sans autoencodeur\n",
    "- Comparer avec approche classique kmeans \n",
    "- Generer dataset syntethique couleur forme unique position variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierre\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, time, os, warnings \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "#import cv2\n",
    "# pip3 install http://download.pytorch.org/whl/cu90/torch-0.4.1-cp36-cp36m-win_amd64.whl\n",
    "# pip3 install torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jpg files in train dataset: 2000\n",
      "The number of jpg files in test dataset: 200\n"
     ]
    }
   ],
   "source": [
    "## The location of the train images\n",
    "dir_train_jpg = \"./Data/Circle_Dataset/train/images/\"\n",
    "## The location of the test images\n",
    "dir_test_jpg = \"./Data/Circle_Dataset/test/images/\"\n",
    "## The location of the train caption file\n",
    "dir_text_train = \"./data/Circle_text/circle_annotations_train.txt\"\n",
    "## The location of the test caption file\n",
    "dir_text_test = \"./data/Circle_text/circle_annotations_test.txt\"\n",
    "\n",
    "\n",
    "jpgs = os.listdir(dir_train_jpg)\n",
    "print(\"The number of jpg files in train dataset: {}\".format(len(jpgs)))\n",
    "jpgs = os.listdir(dir_test_jpg)\n",
    "print(\"The number of jpg files in test dataset: {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique file names : 2000\n",
      "The distribution of the number of captions for each image:\n",
      "The number of unique file names : 200\n",
      "The distribution of the number of captions for each image:\n"
     ]
    }
   ],
   "source": [
    "## read in the Flickr caption data\n",
    "def annotations_reading(dir_text):\n",
    "    file = open(dir_text,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    datatxt = []\n",
    "    for line in text.split('\\n'):\n",
    "        col = line.split('\\t')\n",
    "        if len(col) == 1:\n",
    "            continue\n",
    "        w = col[0].split(\"#\")\n",
    "        datatxt.append(w + [col[1].lower()])\n",
    "\n",
    "\n",
    "    df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
    "\n",
    "    uni_filenames = np.unique(df_txt.filename.values)\n",
    "    print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
    "    print(\"The distribution of the number of captions for each image:\")\n",
    "    Counter(Counter(df_txt.filename.values).values())\n",
    "    \n",
    "    return df_txt, uni_filenames\n",
    "\n",
    "df_txt_train, uni_filenames_train = annotations_reading(dir_text_train)\n",
    "df_txt_test, uni_filenames_test = annotations_reading(dir_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7aad200747a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnpix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnpix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "npix = 224\n",
    "target_size = (npix,npix,3)\n",
    "\n",
    "def get_random_pictures(n_pictures):\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,20))\n",
    "    for jpgfnm in uni_filenames[np.random.randint(0,uni_filenames.shape[0], size=n_pictures)]:\n",
    "        filename = dir_Flickr_jpg + '/' + jpgfnm\n",
    "        captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n",
    "        image_load = load_img(filename, target_size=target_size)\n",
    "        \n",
    "        ax = fig.add_subplot(n_pictures,2,count,xticks=[],yticks=[])\n",
    "        ax.imshow(image_load)\n",
    "        count += 1\n",
    "\n",
    "        ax = fig.add_subplot(n_pictures,2,count)\n",
    "        plt.axis('off')\n",
    "        ax.plot()\n",
    "        ax.set_xlim(0,1)\n",
    "        ax.set_ylim(0,len(captions))\n",
    "        for i, caption in enumerate(captions):\n",
    "            ax.text(0,i,caption,fontsize=20)\n",
    "        count += 1\n",
    "    plt.show()\n",
    "    \n",
    "get_random_pictures(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    embeddings = np.array([model.word_vec(word) for word in sentence.split() if word in model.wv.vocab.keys()])\n",
    "    if embeddings.ndim == 2:\n",
    "        return np.sum(embeddings.T,axis=1)\n",
    "    elif embeddings.ndim == 1:\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300-SLIM.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_embedding('This is a test', text_model).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu' #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for param in image_model.parameters():\n",
    "    param.requires_grad = False\n",
    "image_model = image_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # interactive mode\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './Data/Circle_Dataset/'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "\n",
    "\n",
    "def classes_generation(dataset):\n",
    "    \n",
    "    if dataset == 'train':\n",
    "        image_datasets[dataset].classes = uni_filenames_train\n",
    "    elif dataset == 'test':\n",
    "        image_datasets[dataset].classes = uni_filenames_test\n",
    "        \n",
    "    class_names = image_datasets[dataset].classes\n",
    "  \n",
    "    image_datasets[dataset].class_to_idx = {image_datasets[dataset].classes[i]: i for i in range(len(image_datasets[dataset].classes))}\n",
    "\n",
    "    for i, (image, class_name) in enumerate(image_datasets[dataset].imgs):\n",
    "        filename  = image_datasets[dataset].class_to_idx[image.replace('./Data/Circle_Dataset/'+dataset+'\\\\images\\\\','')]\n",
    "        if dataset == 'train':\n",
    "            image_datasets[dataset].imgs[i] = (image, list(df_txt_train[df_txt_train[\"filename\"]==class_names[filename]]['caption'].values))\n",
    "        elif dataset == 'test':\n",
    "            image_datasets[dataset].imgs[i] = (image, list(df_txt_test[df_txt_test[\"filename\"]==class_names[filename]]['caption'].values))\n",
    "\n",
    "classes_generation('train')\n",
    "classes_generation('test')\n",
    "\n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARcAAAEICAYAAAB1U7CaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26aa0266470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "classes = [x[0] for x in classes]\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_representations(dataset):\n",
    "    if dataset == 'train':\n",
    "        images= torch.zeros([df_txt_train.shape[0], 1000], dtype=torch.float)\n",
    "        captions = torch.zeros([df_txt_train.shape[0], 300], dtype=torch.float)\n",
    "    elif dataset == 'test':\n",
    "        images= torch.zeros([df_txt_test.shape[0], 1000], dtype=torch.float)\n",
    "        captions = torch.zeros([df_txt_test.shape[0], 300], dtype=torch.float)\n",
    "        \n",
    "    for i, (image, labels) in enumerate(dataloaders[dataset]):\n",
    "        labels = [x[0] for x in labels]\n",
    "        im = image_model(image)\n",
    "        for j in range(len(labels)):\n",
    "            capt = get_sentence_embedding(labels[j], text_model)\n",
    "            if capt.size == 300:\n",
    "                # a modifier pour ne pas sauvegarder 5x la meme image\n",
    "                images[i+j] = im\n",
    "                captions[i+j] = torch.from_numpy(capt)\n",
    "    return images, captions\n",
    "\n",
    "images_train, captions_train = generate_representations('train')\n",
    "images_test, captions_test = generate_representations('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, n_inp, n_hidden):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(n_inp, n_hidden)\n",
    "        self.decoder = nn.Linear(n_hidden, n_inp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x)) # relu or sigmoid\n",
    "        decoded = F.sigmoid(self.decoder(encoded))\n",
    "        return encoded, decoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "# BATCH_SIZE = 5 a intégrer\n",
    "N_INP = 1000 + 300 # Image representation + Word2Vec\n",
    "N_HIDDEN = 2000\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "use_kl = False\n",
    "\n",
    "# If using KL-divergence\n",
    "BETA = 3 # Importance of the KL-divergence penalty term\n",
    "RHO = 0.01 # Target sparsity\n",
    "\n",
    "if use_kl :\n",
    "    rho = torch.FloatTensor([RHO for _ in range(N_HIDDEN)]).unsqueeze(0)\n",
    "\n",
    "# If using L1 regularization\n",
    "LAMBDA = 1e-8 # L1 penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_encoder = SparseAutoencoder(N_INP, N_HIDDEN)\n",
    "optimizer = optim.Adam(auto_encoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    '''\n",
    "    args:\n",
    "        2 tensors `p` and `q`\n",
    "    returns:\n",
    "        kl divergence between the softmax of `p` and `q`\n",
    "    '''\n",
    "    p = F.softmax(p)\n",
    "    q = F.softmax(q)\n",
    "\n",
    "    s1 = torch.sum(p * torch.log(p / q))\n",
    "    s2 = torch.sum((1 - p) * torch.log((1 - p) / (1 - q)))\n",
    "\n",
    "    return s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_penalty(var):\n",
    "    return torch.abs(var).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1300])\n",
      "Epoch: [  1], Loss: 0.0298\n",
      "Reconstruction mean squared error :  tensor(0.0250, grad_fn=<MseLossBackward>)\n",
      "Average sparsity :  tensor(239.1169, grad_fn=<DivBackward0>)\n",
      "Epoch: [  2], Loss: 0.0163\n",
      "Reconstruction mean squared error :  tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "Average sparsity :  tensor(168.2714, grad_fn=<DivBackward0>)\n",
      "Epoch: [  3], Loss: 0.0115\n",
      "Reconstruction mean squared error :  tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "Average sparsity :  tensor(293.5846, grad_fn=<DivBackward0>)\n",
      "Epoch: [  4], Loss: 0.0086\n",
      "Reconstruction mean squared error :  tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "Average sparsity :  tensor(295.2657, grad_fn=<DivBackward0>)\n",
      "Epoch: [  5], Loss: 0.0065\n",
      "Reconstruction mean squared error :  tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "Average sparsity :  tensor(243.7818, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.cat((images_train, captions_train), 1)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "def normalize(dataset):\n",
    "    '''Normalization between 0 and 1'''\n",
    "    for i, row in enumerate(dataset):\n",
    "        min = torch.min(row)\n",
    "        max = torch.max(row)\n",
    "    \n",
    "        dataset[i] = (row - min)/(max - min)\n",
    "    return dataset\n",
    "\n",
    "dataset = normalize(dataset)\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    encoded, decoded = auto_encoder(dataset)\n",
    "\n",
    "    mse_loss = F.mse_loss(decoded, dataset)\n",
    "\n",
    "    rho_hat = torch.sum(encoded, dim=0, keepdim=True)\n",
    "    \n",
    "    if use_kl:\n",
    "        sparsity_penalty = BETA * kl_divergence(rho, rho_hat)\n",
    "        loss = mse_loss + sparsity_penalty\n",
    "    else:\n",
    "        l1_reg = LAMBDA * l1_penalty(encoded)\n",
    "        loss = mse_loss + l1_reg\n",
    "    \n",
    "    optimizer.zero_grad() # Clears the gradient\n",
    "    loss.backward() # Computes gradient\n",
    "    optimizer.step() # Update rule\n",
    "\n",
    "    print(\"Epoch: [%3d], Loss: %.4f\" %(epoch + 1, loss.data))\n",
    "    print(\"Reconstruction mean squared error : \", mse_loss)\n",
    "    print(\"Average sparsity : \",  rho_hat.sum()/N_HIDDEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 elements different from 0\n",
      "Reconstruction mean squared error on test :  tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor([[0.0565, 0.0396, 0.0142,  ..., 0.0426, 0.0302, 0.0419],\n",
      "        [0.0418, 0.0330, 0.0093,  ..., 0.0350, 0.0262, 0.0400],\n",
      "        [0.0339, 0.0240, 0.0031,  ..., 0.0094, 0.0069, 0.0302],\n",
      "        ...,\n",
      "        [0.0533, 0.0389, 0.0085,  ..., 0.0224, 0.0153, 0.0432],\n",
      "        [0.0526, 0.0345, 0.0132,  ..., 0.0263, 0.0262, 0.0341],\n",
      "        [0.0172, 0.0067, 0.0287,  ..., 0.0138, 0.0162, 0.0221]],\n",
      "       grad_fn=<AbsBackward>)\n",
      "tensor(10021.3428, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "dataset_test = torch.cat((images_test, captions_test), 1)\n",
    "\n",
    "dataset_test = normalize(dataset_test)\n",
    "\n",
    "encoded, decoded = auto_encoder(dataset_test)\n",
    "mse_loss_test = F.mse_loss(decoded, dataset_test)\n",
    "\n",
    "i=0\n",
    "for e in encoded[0]:\n",
    "    if e > 1e-3:\n",
    "        i+=1\n",
    "print(i, \"elements different from 0\")\n",
    "print(\"Reconstruction mean squared error on test : \", mse_loss_test)\n",
    "print(torch.abs(dataset_test-decoded))\n",
    "print(torch.sum(torch.abs(dataset_test-decoded)))\n",
    "# divide by norm for percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoassociative memory - Hopfield Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get autoencoder's hidden representations\n",
    "x = torch.cat((images_train, captions_train), 1)\n",
    "encoded, _ = auto_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_weights(learning_patterns):\n",
    "    \n",
    "    w = (np.transpose(learning_patterns) @ learning_patterns) / learning_patterns.shape[0]\n",
    "    np.fill_diagonal(w, 0)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy(x, w):\n",
    "    \n",
    "    energy = 0\n",
    "    for i in range(w.shape[0]):\n",
    "        for j in range(w.shape[1]):\n",
    "            energy -= w[i, j]*x[i]*x[j]\n",
    "            \n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_rule_little_model(x, w):\n",
    "\n",
    "    x = x @ w\n",
    "    #x = np.where(x>=0, 1, -1)\n",
    "    #x = np.tanh(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(x, w):\n",
    "    \n",
    "    print(\"Initial energy : \", energy(x, w))\n",
    "    x_new = update_rule_little_model(x, w)\n",
    "    i = 0 # To stop if no convergence\n",
    "    print(\"Energy : \", energy(x_new, w))\n",
    "    while(np.array_equal(x_new, x) == False and i<10):\n",
    "        x = x_new\n",
    "        x_new = update_rule_little_model(x, w)\n",
    "        print(\"Energy : \", energy(x_new, w))\n",
    "        i+=1\n",
    "        \n",
    "    return x_new, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]\n",
      " [0.        2.2600746 0.        ... 0.        0.        0.       ]\n",
      " [0.        0.3181247 0.        ... 0.        0.        0.       ]\n",
      " [0.        0.        0.        ... 0.        0.        0.       ]]\n",
      "Initial energy :  -8550.918475982548\n",
      "Energy :  -51848276.32394375\n",
      "Energy :  -375481217982.0229\n",
      "Energy :  -2753086955377024.0\n",
      "Energy :  -2.020736350760598e+19\n",
      "Energy :  -1.4833648162475466e+23\n",
      "Energy :  -1.0889102808537072e+27\n",
      "Energy :  -7.993498971463502e+30\n",
      "Energy :  -5.8678885952541e+34\n",
      "Energy :  -4.307515011647521e+38\n",
      "Energy :  -inf\n",
      "Energy :  -inf\n",
      "(1800,)\n",
      "[0.        2.2600746 0.        ... 0.        0.        0.       ]\n",
      "[0.000000e+00 8.860412e+20 0.000000e+00 ... 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00]\n",
      "1.7482015e+23\n"
     ]
    }
   ],
   "source": [
    "encoded = encoded.detach().numpy()\n",
    "print(encoded)\n",
    "w = calculate_weights(learning_patterns=encoded)\n",
    "x, _ = recall(encoded[2,:], w)\n",
    "print(x.shape)\n",
    "print(encoded[2,:])\n",
    "print(x)\n",
    "print(np.sum(np.abs(x-encoded[2,:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
